{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextblob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextBlob\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data files\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load spaCy's English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# List of stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Preprocessing function to clean and prepare the text\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters, digits, and punctuations\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords and lemmatize the words\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Function to extract preferences from reviews\n",
    "def extract_preferences(reviews):\n",
    "    preferences = []\n",
    "\n",
    "    for review in reviews:\n",
    "        # Step 1: Preprocess the review\n",
    "        cleaned_review = preprocess_text(review)\n",
    "\n",
    "        # Step 2: Apply spaCy NLP processing to extract noun chunks (food-related preferences)\n",
    "        doc = nlp(cleaned_review)\n",
    "        noun_chunks = [chunk.text for chunk in doc.noun_chunks]\n",
    "\n",
    "        # Step 3: Sentiment Analysis using TextBlob\n",
    "        blob = TextBlob(review)\n",
    "        sentiment = blob.sentiment.polarity  # Range from -1 (negative) to 1 (positive)\n",
    "\n",
    "        # Step 4: Categorize the feedback (Positive, Negative, Neutral)\n",
    "        if sentiment > 0:\n",
    "            sentiment_label = \"Positive Feedback\"\n",
    "        elif sentiment < 0:\n",
    "            sentiment_label = \"Negative Feedback\"\n",
    "        else:\n",
    "            sentiment_label = \"Neutral Feedback\"\n",
    "\n",
    "        # Step 5: Capture extracted preferences and sentiment\n",
    "        preferences.append({\n",
    "            \"review\": review,\n",
    "            \"cleaned_review\": cleaned_review,\n",
    "            \"noun_chunks\": noun_chunks,\n",
    "            \"sentiment\": sentiment_label\n",
    "        })\n",
    "\n",
    "    return preferences\n",
    "\n",
    "# Example food-related reviews\n",
    "reviews = [\n",
    "    \"The pizza was delicious, especially the crust which was crispy and flavorful.\",\n",
    "    \"I hate the bland taste of the soup, it was too salty for my liking.\",\n",
    "    \"The burger was juicy but the fries were soggy and undercooked.\",\n",
    "    \"Absolutely loved the ice cream! So creamy and rich in flavor, will definitely buy again.\"\n",
    "]\n",
    "\n",
    "# Extract preferences\n",
    "preferences = extract_preferences(reviews)\n",
    "\n",
    "# Display extracted preferences\n",
    "for i, pref in enumerate(preferences):\n",
    "    print(f\"\\nReview {i + 1}: {pref['review']}\")\n",
    "    print(f\"Cleaned Review: {pref['cleaned_review']}\")\n",
    "    print(f\"Noun Phrases Extracted: {pref['noun_chunks']}\")\n",
    "    print(f\"Sentiment: {pref['sentiment']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "from dash import dcc, html, dash_table\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from jupyter_dash import JupyterDash  # Import JupyterDash instead of Dash\n",
    "\n",
    "\n",
    "# Sample data with various cuisines\n",
    "recipes = pd.DataFrame({\n",
    "    \"title\": [\n",
    "        \"Beef Stroganoff\", \"Margherita Pizza\", \"Penne Arrabbiata\",\n",
    "        \"Burritos\", \"Salsa\", \"Quesadillas\",\n",
    "        \"General Tso's Chicken\", \"Mapo Tofu\", \"Egg Rolls\",\n",
    "        \"Lamb Rogan Josh\", \"Palak Paneer\", \"Biryani\"\n",
    "    ],\n",
    "    \"ingredients\": [\n",
    "        \"beef, mushrooms, sour cream\", \"pizza dough, mozzarella, basil\", \"penne, tomatoes, chili flakes\",\n",
    "        \"tortillas, rice, beans, cheese\", \"tomatoes, onions, cilantro, lime\", \"tortillas, cheese, chicken\",\n",
    "        \"chicken, soy sauce, hoisin sauce\", \"tofu, ground pork, chili bean paste\", \"egg roll wrappers, cabbage, pork\",\n",
    "        \"lamb, yogurt, spices\", \"spinach, paneer, cream\", \"rice, chicken, saffron, spices\"\n",
    "    ],\n",
    "    \"directions\": [\n",
    "        \"Cook beef with mushrooms, add sour cream\", \"Bake pizza dough with toppings\", \"Cook penne, mix with spicy tomato sauce\",\n",
    "        \"Fill tortillas with rice, beans, and cheese\", \"Mix tomatoes, onions, cilantro, and lime\", \"Fill tortillas with cheese and chicken, cook\",\n",
    "        \"Fry chicken, mix with sauce\", \"Cook tofu and pork with chili bean paste\", \"Fill wrappers with cabbage and pork, fry\",\n",
    "        \"Cook lamb with yogurt and spices\", \"Cook spinach, mix with paneer and cream\", \"Cook rice with chicken and spices\"\n",
    "    ],\n",
    "    \"cuisine\": [\n",
    "        \"Russian\", \"Italian\", \"Italian\",\n",
    "        \"Mexican\", \"Mexican\", \"Mexican\",\n",
    "        \"Chinese\", \"Chinese\", \"Chinese\",\n",
    "        \"Indian\", \"Indian\", \"Indian\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Bar chart for ingredient frequency\n",
    "ingredient_counts = recipes['ingredients'].str.split(', ').explode().value_counts()\n",
    "ingredient_bar_chart = px.bar(\n",
    "    ingredient_counts,\n",
    "    x=ingredient_counts.index,\n",
    "    y=ingredient_counts.values,\n",
    "    labels={'x': 'Ingredients', 'y': 'Frequency'},\n",
    "    title='Frequency of Ingredients in Recipes'\n",
    ")\n",
    "\n",
    "# Pie chart for cuisine distribution\n",
    "cuisine_pie_chart = px.pie(\n",
    "    recipes,\n",
    "    names='cuisine',\n",
    "    title='Distribution of Cuisines in Recipes'\n",
    ")\n",
    "\n",
    "# Dash app setup\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Recipe Dashboard\"),\n",
    "\n",
    "    html.H2(\"Recipe Table\"),\n",
    "    dash_table.DataTable(\n",
    "        data=recipes.to_dict('records'),\n",
    "        columns=[{\"name\": col, \"id\": col} for col in recipes.columns],\n",
    "        style_table={'overflowX': 'auto'}\n",
    "    ),\n",
    "\n",
    "    html.H2(\"Ingredient Frequency\"),\n",
    "    dcc.Graph(figure=ingredient_bar_chart),\n",
    "\n",
    "    html.H2(\"Cuisine Distribution\"),\n",
    "    dcc.Graph(figure=cuisine_pie_chart),\n",
    "])\n",
    "\n",
    "# Run the Dash app\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True, use_reloader=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "food_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
